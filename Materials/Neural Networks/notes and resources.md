# Notes and Resources

- Nielsen, [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)
- Harvard AM207, [MLP as universal approximator](http://am207.info/wiki/FuncTorch.html)
- Stanford CS231n, [Neural Networks](https://cs231n.github.io/neural-networks-1/)
- Wiecki, [Hierarchical Bayesian Neural Networks with Informative Priors](https://twiecki.io/blog/2018/08/13/hierarchical_bayesian_neural_network/)
- Boney, [Theoretical Motivations for Deep Learning](https://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html)
- Skalski,
    - [Deep Dive into Math Behind Deep Networks](https://towardsdatascience.com/https-medium-com-piotr-skalski92-deep-dive-into-deep-networks-math-17660bc376ba)
    - [Preventing Deep Neural Network from Overfitting](https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a)
    - [Letâ€™s code a Neural Network in plain NumPy](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795)
    - [How to train Neural Network faster with optimizers?](https://towardsdatascience.com/how-to-train-neural-network-faster-with-optimizers-d297730b3713)
