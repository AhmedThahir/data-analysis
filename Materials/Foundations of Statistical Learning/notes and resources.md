# Notes and Resources

- Abu-Mostafa, [Learning from Data](https://work.caltech.edu/telecourse.html) 
    - Lectures 1 - 8
- Vapnik, [Statistical Theory of Learning](https://www.youtube.com/watch?v=Ow25mjFjSmg) [`video`]
- Rosenberg, [Foundations of Machine Learning](https://bloomberg.github.io/foml/#home)
    - Lecture 3
- Daum√© III, [A Course in Machine Learning](http://ciml.info/)
    - Chapter 2, 9, 12
- Dalpiaz, [R for Statistical Learning](https://daviddalpiaz.github.io/r4sl/)
    - Chapter 8
- Fortmann-Roe, [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)
- Olah, [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/)
- The RL Probabilist, [KL Divergence for Machine Learning](https://dibyaghosh.com/blog/probability/kldivergence.html)
- Stanford Encyclopedia of Philosophy
    - [Philosophy of Statistics](https://plato.stanford.edu/entries/statistics/)
    - [Bayesian Epistemology](https://plato.stanford.edu/entries/epistemology-bayesian/)
- VanderPlas, [Frequentism and Bayesianism](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)
